}
library(RJSONIO)
getBCPricesURL <- function(dateStart, daysBack)
{
baseUrl<-"https://api.coindesk.com/v1/bpi/historical/close.json"
getPricesURL<-paste(baseUrl, "?index=USD&", "start=",format(as.Date(dateStart)-daysBack, "%Y-%m-%d"),"&end=",format(as.Date(dateStart), "%Y-%m-%d"), sep='')
}
source('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
getGoogleURL <- function(search.term, dateRef, daysBack,num=100, domain = '.com', quotes=TRUE)
{
dateEnd<- format(as.Date(dateRef), format="%m/%d/%Y")
dateStart<- format(as.Date(dateRef)-daysBack, format="%m/%d/%Y")
search.term <- gsub(' ', '%20', search.term)
if(quotes) search.term <- paste('%22', search.term, '%22', sep='')
getGoogleURL <- paste('http://www.google', domain, '/search?q=',
search.term,
'&num=', num, '&tbs=cdr:1,cd_min:',dateStart,',cd_max:',dateEnd,',sbd:1',
'&tbm=nws&source=lnt'
, sep='')
}
library(httr)
library('rvest')
library(RSentiment)
getSentimentSummary <- function(search.term, dateRef, daysBack)
{
#Specifying the url for desired website to be scrapped
url <- getGoogleURL(search.term = search.term, dateRef = dateRef, daysBack = daysBack)
#getSentimentSummary <- url
#Reading the HTML code from the website
#webpage <- read_html(url)
print(url)
#Using CSS selectors to scrape the titles
#title_data_html <- html_nodes(webpage,'.l _PMs')
#Converting the titles data to text
#title_data <- html_text(title_data_html)
#Call GET and add a useragent to ensure that the tbs is used.
response <- GET(url, add_headers('user-agent' = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36'))
res <- content(response)
print(res)
#Using CSS selectors to scrape the descrip
descrip_data_html <- html_nodes(res,'.st')
#Converting the descrip data to text
descrip_data <- html_text(descrip_data_html)
#print(descrip_data)
sentimentCollection <- calculate_score(descrip_data)
remove99<-c(99)
sentimentCollectionClean<-sentimentCollection [! sentimentCollection %in% remove99]
summaryValues <- summary(sentimentCollectionClean);
getSentimentSummary<-summaryValues
}
library(RJSONIO)
getBCPricesURL <- function(dateStart, daysBack)
{
baseUrl<-"https://api.coindesk.com/v1/bpi/historical/close.json"
getPricesURL<-paste(baseUrl, "?index=USD&", "start=",format(as.Date(dateStart)-daysBack, "%Y-%m-%d"),"&end=",format(as.Date(dateStart), "%Y-%m-%d"), sep='')
}
getSentimentAndPricesForPeriod <- function(dateStart, daysBack)
{
url<-getBCPricesURL(dateStart=dateStart, daysBack=daysBack)
dataOut<-fromJSON(url)
datesBuilts  =  seq(as.Date(dateStart)-daysBack, as.Date(dateStart), by="days")
sentimentData<-mapply(getSentimentSummary, datesBuilts, search.term="bitcoin", daysBack=1)
}
sentimentoutputsingle <-getSentimentSummary(search.term="bitcoin","2017-09-07", 2)
sentimentoutputsingle
outputsfull<-getSentimentAndPricesForPeriod("2017-10-02", 2)
sentimentoutputsingle <-getSentimentSummary(search.term="bitcoin","2017-09-07", 1)
sentimentoutputsingle
outputsfull<-getSentimentAndPricesForPeriod("2017-10-02", 2)
getSentimentAndPricesForPeriod <- function(dateStart, daysBack)
{
url<-getBCPricesURL(dateStart=dateStart, daysBack=daysBack)
print("pricesUrl: ")
print(url)
dataOut<-fromJSON(url)
datesBuilts  =  seq(as.Date(dateStart)-daysBack, as.Date(dateStart), by="days")
sentimentData<-mapply(getSentimentSummary, datesBuilts, search.term="bitcoin", daysBack=1)
}
outputsfull<-getSentimentAndPricesForPeriod("2017-10-02", 2)
outputsfull<-getSentimentAndPricesForPeriod("2017-10-02", 2)
outputsfull
url<-getBCPricesURL(dateStart="2017-10-02", daysBack=1)
print("pricesUrl: ")
print(url)
dataOut<-fromJSON(url)
dataOut
outputsfull
dataOut$bpi
outputsfull[,3]
prices<-dataOut$bpi
prices[1,]
prices[1]
prices[,1]
t(outputsfull)
merge(prices, outputsfull)
rbind(prices, outputsfull)
rbind(prices, t(outputsfull))
install.packages("twitteR")
library(twitteR)
searchTwitter(searchString="bitcoin", since="2017-09-01", until="2017-09-07")
library(twitteR)
setup_twitter_oauth("RqxwQvin3yeZrQfCrrRA", "MimaKfV5IQQdCHabzxkZXan2f0W5W79IZsLPiusbfs", access_token=NULL, access_secret=NULL)
library(twitteR)
setup_twitter_oauth("RqxwQvin3yeZrQfCrrRA", "MimaKfV5IQQdCHabzxkZXan2f0W5W79IZsLPiusbfs", access_token=NULL, access_secret=NULL)
searchTwitter(searchString="bitcoin", since="2017-09-01", until="2017-09-07")
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
prices
sentimentData
outputsToWrite
outputToWrite
if (!is.null(fileInput)) {
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
if (!is.null(fileInput)) {
if (!is.null(fileInput)) {
outputToWrite <- merge(fileInput, outputToWrite)
}
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
j
;
library(twitteR)
setup_twitter_oauth("RqxwQvin3yeZrQfCrrRA", "MimaKfV5IQQdCHabzxkZXan2f0W5W79IZsLPiusbfs", access_token=NULL, access_secret=NULL)
searchTwitter(searchString="bitcoin", since="2017-09-01", until="2017-09-07")
setup_twitter_oauth("RqxwQvin3yeZrQfCrrRA", "MimaKfV5IQQdCHabzxkZXan2f0W5W79IZsLPiusbfs", access_token=NULL, access_secret=NULL)
library(twitteR)
setup_twitter_oauth("RqxwQvin3yeZrQfCrrRA", "MimaKfV5IQQdCHabzxkZXan2f0W5W79IZsLPiusbfs", access_token="210046899-AQfbSWGL6hMkClufkFOadPJJLbJ0vy4F2zswyqf9", access_secret="wqUDuMl552R6EYTLjVyEHrtkTS4l0BZdIvtXLil832Cuq")
searchTwitter(searchString="bitcoin", since="2017-09-01", until="2017-09-07")
getTwitterURL <- function(search.term, dateRef, daysBack, domain = '.com', quotes=TRUE)
{
dateEnd<- format(as.Date(dateRef), format="%Y-%m-%d")
dateStart<- format(as.Date(dateRef)-daysBack, format="%Y-%m-%d")
search.term <- gsub(' ', '%20', search.term)
if(quotes) search.term <- paste('%22', search.term, '%22', sep='')
getTwitterURL <- paste('https://twitter', domain, '/search?f=tweets&q=',
search.term,
'since%3A',dateStart,'%20%20until%3A',dateEnd,
'&src=typd'
, sep='')
}
getTwitterUrl("bitcoin","2017-09-10", 5)
getTwitterURL <- function(search.term, dateRef, daysBack, domain = '.com', quotes=TRUE)
{
dateEnd<- format(as.Date(dateRef), format="%Y-%m-%d")
dateStart<- format(as.Date(dateRef)-daysBack, format="%Y-%m-%d")
search.term <- gsub(' ', '%20', search.term)
if(quotes) search.term <- paste('%22', search.term, '%22', sep='')
getTwitterURL <- paste('https://twitter', domain, '/search?f=tweets&q=',
search.term,
'since%3A',dateStart,'%20%20until%3A',dateEnd,
'&src=typd'
, sep='')
}
getTwitterUrl("bitcoin","2017-09-10", 5)
getTwitterURL("bitcoin","2017-09-10", 5)
print(getTwitterURL("bitcoin","2017-09-10", 5))
url<-getTwitterURL("bitcoin","2017-09-10", 5)
webpage <- read_html(url)
library('rvest')
url<-getTwitterURL("bitcoin","2017-09-10", 5)
webpage <- read_html(url)
descrip_data_html <- html_nodes(res,'.tweet-text')
url<-getTwitterURL("bitcoin","2017-09-10", 5)
webpage <- read_html(url)
descrip_data_html <- html_nodes(webpage,'.tweet-text')
descrip_data <- html_text(descrip_data_html)
descrip_data
webpage
descrip_data_html <- html_nodes(webpage,'div')
descrip_data <- html_text(descrip_data_html)
descrip_data
url<-getTwitterURL("bitcoin","2017-09-10", 5)
webpage <- read_html(url)
descrip_data_html <- html_nodes(webpage,'.tweet-text')
descrip_data <- html_text(descrip_data_html)
descrip_data
response <- GET(url, add_headers('user-agent' = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36'))
library(RCurl)
library(httr)
library('rvest')
url<-getTwitterURL("bitcoin","2017-09-10", 5)
response <- GET(url, add_headers('user-agent' = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36'))
webpage <- content(response)
descrip_data_html <- html_nodes(webpage,'.tweet-text')
descrip_data <- html_text(descrip_data_html)
descrip_data
getGoogleURL <- function(search.term, dateRef, daysBack,num=100, domain = '.com', quotes=TRUE)
{
dateEnd<- format(as.Date(dateRef), format="%m/%d/%Y")
dateStart<- format(as.Date(dateRef)-daysBack, format="%m/%d/%Y")
search.term <- gsub(' ', '%20', search.term)
if(quotes) search.term <- paste('%22', search.term, '%22', sep='')
getGoogleURL <- paste('http://www.google', domain, '/search?q=',
search.term,
'&num=', num, '&tbs=cdr:1,cd_min:',dateStart,',cd_max:',dateEnd,',sbd:1',
'&tbm=nws&source=lnt'
, sep='')
}
library(httr)
library('rvest')
library(RSentiment)
getSentimentSummary <- function(search.term, dateRef, daysBack)
{
#Specifying the url for desired website to be scrapped
url <- getGoogleURL(search.term = search.term, dateRef = dateRef, daysBack = daysBack)
#getSentimentSummary <- url
#Reading the HTML code from the website
#webpage <- read_html(url)
print("newsUrl: ")
print(url)
#Using CSS selectors to scrape the titles
#title_data_html <- html_nodes(webpage,'.l _PMs')
#Converting the titles data to text
#title_data <- html_text(title_data_html)
#Call GET and add a useragent to ensure that the tbs is used.
response <- GET(url, add_headers('user-agent' = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36'))
res <- content(response)
#print(res)
#Using CSS selectors to scrape the descrip
descrip_data_html <- html_nodes(res,'.st')
#Converting the descrip data to text
descrip_data <- html_text(descrip_data_html)
#print(descrip_data)
sentimentCollection <- calculate_score(descrip_data)
remove99<-c(99)
sentimentCollectionClean<-sentimentCollection [! sentimentCollection %in% remove99]
summaryValues <- summary(sentimentCollectionClean);
labels(summaryValues)<-labels(summaryValues)+"NEWS"
getSentimentSummary<-summaryValues
}
library(RJSONIO)
getBCPricesURL <- function(dateStart, daysBack)
{
baseUrl<-"https://api.coindesk.com/v1/bpi/historical/close.json"
getPricesURL<-paste(baseUrl, "?index=USD&", "start=",format(as.Date(dateStart)-daysBack, "%Y-%m-%d"),"&end=",format(as.Date(dateStart), "%Y-%m-%d"), sep='')
}
getSentimentAndPricesForPeriod <- function(dateStart, daysBack)
{
url<-getBCPricesURL(dateStart=dateStart, daysBack=daysBack)
print("pricesUrl: ")
print(url)
dataOut<-fromJSON(url)
datesBuilts  =  seq(as.Date(dateStart)-daysBack, as.Date(dateStart), by="days")
sentimentData<-mapply(getSentimentSummary, datesBuilts, search.term="bitcoin", daysBack=1, USE.NAMES = TRUE)
fulloutput<- rbind(prices, sentimentData)
}
appendSentimentAndPricesForPeriod<- function(dateStart, daysBack, filename)
{
if(file.exists(filename))
{
fileInput <- read.csv(filename, header=TRUE)
}
outputToWrite <-getSentimentAndPricesForPeriod(dateStart, daysBack)
if (!is.null(fileInput)) {
outputToWrite <- merge(fileInput, outputToWrite)
}
write.csv(outputToWrite, file=filename, header=TRUE)
appendSentimentAndPricesForPeriod<-fileInput
}
appendSentimentAndPricesForPeriod("2017-10-02", 2, "priceindexstore.csv")
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
summaryValues
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
summaryValues
summaryValues
getSentimentSummary<-summaryValues
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
summaryValues
summaryValues
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
sentimentData
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
fileInput
outputToWrite <-getSentimentAndPricesForPeriod(dateStart, daysBack)
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
outputToWrite
function(search.term, dateRef, daysBack)
{
#Specifying the url for desired website to be scrapped
url <- getGoogleURL(search.term = search.term, dateRef = dateRef, daysBack = daysBack)
#getSentimentSummary <- url
#Reading the HTML code from the website
#webpage <- read_html(url)
print("newsUrl: ")
print(url)
#Using CSS selectors to scrape the titles
#title_data_html <- html_nodes(webpage,'.l _PMs')
#Converting the titles data to text
#title_data <- html_text(title_data_html)
#Call GET and add a useragent to ensure that the tbs is used.
response <- GET(url, add_headers('user-agent' = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36'))
res <- content(response)
#print(res)
#Using CSS selectors to scrape the descrip
descrip_data_html <- html_nodes(res,'.st')
#Converting the descrip data to text
descrip_data <- html_text(descrip_data_html)
#print(descrip_data)
sentimentCollection <- calculate_score(descrip_data)
remove99<-c(99)
sentimentCollectionClean<-sentimentCollection [! sentimentCollection %in% remove99]
summaryValues <- summary(sentimentCollectionClean);
#labels(summaryValues)<-labels(summaryValues)+"NEWS"
getSentimentSummary<-summaryValues
}
sentimentCollection <- calculate_score(descrip_data)
sentimentCollection <- calculate_score(descrip_data)
source('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
source('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
robin<-summary(sentimentCollectionClean)
robin
robin[0]
robin[1]
labels(robin)<-c("a", "b", "c","d","e","f", "g")
labels<-c("a", "b", "c","d","e","f", "g")
labels(robin)
factor(robin, levels=robin,labels=labels)
labels<-c("a", "b", "c","d","e","f")
factor(robin, levels=robin,labels=labels)
names
names(robin)
names(robin) <- labels
robin
rep("rob", 10)
labels+rob
source('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
source('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
source('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
labels<-c(prefix+"Min", "FirstQ", "Med","Mean","ThirdQ","Max")
prefix <- paste(labels,prefix, sep='_')
labels<-c("Min", "FirstQ", "Med","Mean","ThirdQ","Max")
prefix <- paste(labels,prefix, sep='_')
labels<-c("Min", "FirstQ", "Med","Mean","ThirdQ","Max")
labels <- paste(labels,"NEWS", sep='_')
labels
labels<-c("Min", "FirstQ", "Med","Mean","ThirdQ","Max")
labels <- paste("NEWS", labels, sep='_')
labels
source('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
sentimentData
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
sentimentData
sentimentData
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
fulloutput<- rbind(prices, sentimentData)
sentimentData
names(sentimentData)
colnames(sentimentData)
colnames(sentimentData)<-datesBuilts
sentimentData
n
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
googleData
twitterData
rbind(googleData, twitterData)
cbind(googleData, twitterData)
merge(googleData, twitterData)
googleData+ twitterData
c(googleData, twitterData)
seq(as.Date("2017-09-10")-3, as.Date("2017-09-10"), by="days")
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
sentimentData
dateStart <-"2017-10-11"
daysBack<-2
datesBuilts  <-  seq(as.Date(dateStart)-daysBack, as.Date(dateStart), by="days")
datesBuilts
format(datesBuilts, "%Y-%m-%d")
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
outputToWrite
fileInput
outputToWrite
fileInput
outputToWrite
colnames(sentimentData)<- format(datesBuilts, "%Y-%m-%d")
fulloutput<- rbind(prices, sentimentData)
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
outputToWrite
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
sentimentCollectionClean
n
n
appendSentimentAndPricesForPeriod("2017-10-10", 2, "priceindexstore.csv")
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
url
res
selector
html_nodes(res, "div")
install.packages("Quandl")
library(Quandl)
getCommodityPrice<-function(commodity, dateStart, daysBack)
{
#"OPEC/ORB"
getCommodityPrice <- Quandl(commodity, start_date=format(as.Date(dateStart)-daysBack, "%Y-%m-%d"), end_date=format(as.Date(dateStart), "%Y-%m-%d"))
}
getCommodityPrice("OPEC/ORB", "2017-10-10", 5)
print(getCommodityPrice("OPEC/ORB", "2017-10-10", 5))
getCommodityPrice<-function(commodity, dateStart, daysBack)
{
#"OPEC/ORB"
outputData <- Quandl(commodity, start_date=format(as.Date(dateStart)-daysBack, "%Y-%m-%d"), end_date=format(as.Date(dateStart), "%Y-%m-%d"))
colnames(outputData)<-c("Date", commodity)
getCommodityPrice <- outputData
}
print(getCommodityPrice("OPEC/ORB", "2017-10-10", 5))
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
fileInput
outputToWrite
fileInput[1,]
fileInput[,1]
names(fileInput)
rownames(fileInput)
rownames(fileInput)<-fileInput[,1]
fileInput
fileInput[,c(1,3,4,5)]
fileInput[,c(1,3,4)]
subset(fileInput, select=-c(2))
fileInput
subset(fileInput, select=-c(1))
fileInput<-subset(fileInput, select=-c(1))
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
fileInput
outputToWrite
rbind(fileInput, outputToWrite)
cbind(fileInput, outputToWrite)
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
cbind(fileInput, outputToWrite)
getCommodityPrice("LBMA/GOLD", "2017-10-10", 5)
print(getCommodityPrice("LBMA/GOLD", "2017-10-10", 5))
getCommodityPrice<-function(commodity, dateStart, daysBack)
{
#"OPEC/ORB"
#"LBMA/GOLD"
#"EIA/AEO_2016_REF_NO_CPP_PRCE_NA_COMM_NA_ELC_NA_WENWPP_Y13CNTPKWH_A"
#"BITCOINWATCH/MINING"
outputData <- Quandl(commodity, start_date=format(as.Date(dateStart)-daysBack, "%Y-%m-%d"), end_date=format(as.Date(dateStart), "%Y-%m-%d"))
colnames(outputData)<-paste(commodity, colnames(outputData), sep="_")
getCommodityPrice <- outputData
}
print(getCommodityPrice("LBMA/GOLD", "2017-10-10", 5))
print(t(etCommodityPrice("LBMA/GOLD", "2017-10-10", 5))
print(t(getCommodityPrice("LBMA/GOLD", "2017-10-10", 5))
print(t(getCommodityPrice("LBMA/GOLD", "2017-10-10", 5)))
getCommodityPrice<-function(commodity, dateStart, daysBack)
{
#"OPEC/ORB"
#"LBMA/GOLD"
#"EIA/AEO_2016_REF_NO_CPP_PRCE_NA_COMM_NA_ELC_NA_WENWPP_Y13CNTPKWH_A"
#"BITCOINWATCH/MINING"
outputData <- Quandl(commodity, start_date=format(as.Date(dateStart)-daysBack, "%Y-%m-%d"), end_date=format(as.Date(dateStart), "%Y-%m-%d"))
colnames(outputData)<-paste(commodity, colnames(outputData), sep="_")
outputData<-subset(outputData, c(1,2))
transposedOut <- t(outputData)
colnames(transposedOut)<-transposedOut[1,]
transposedOut<-transposedOut[-c(1),]
getCommodityPrice <- transposedOut
}
print(getCommodityPrice("LBMA/GOLD", "2017-10-10", 5))
getCommodityPrice<-function(commodity, dateStart, daysBack)
{
#"OPEC/ORB"
#"LBMA/GOLD"
#"EIA/AEO_2016_REF_NO_CPP_PRCE_NA_COMM_NA_ELC_NA_WENWPP_Y13CNTPKWH_A"
#"BITCOINWATCH/MINING"
outputData <- Quandl(commodity, start_date=format(as.Date(dateStart)-daysBack, "%Y-%m-%d"), end_date=format(as.Date(dateStart), "%Y-%m-%d"))
colnames(outputData)<-paste(commodity, colnames(outputData), sep="_")
outputData<-outputData[,c(1,2)]
transposedOut <- t(outputData)
colnames(transposedOut)<-transposedOut[1,]
transposedOut<-transposedOut[-c(1),]
getCommodityPrice <- transposedOut
}
print(getCommodityPrice("LBMA/GOLD", "2017-10-10", 5))
getCommodityPrice<-function(commodity, dateStart, daysBack)
{
#"OPEC/ORB"
#"LBMA/GOLD"
#"EIA/AEO_2016_REF_NO_CPP_PRCE_NA_COMM_NA_ELC_NA_WENWPP_Y13CNTPKWH_A"
#"BITCOINWATCH/MINING"
outputData <- Quandl(commodity, start_date=format(as.Date(dateStart)-daysBack, "%Y-%m-%d"), end_date=format(as.Date(dateStart), "%Y-%m-%d"))
colnames(outputData)<-paste(commodity, colnames(outputData), sep="_")
outputData<-outputData[,c(1,2)]
transposedOut <- t(outputData)
colnames(transposedOut)<-transposedOut[1,]
transposedOut<-transposedOut[-c(1),]
names(transposedOut)<-c(commodity)
getCommodityPrice <- transposedOut
}
print(getCommodityPrice("LBMA/GOLD", "2017-10-10", 5))
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
OIL
fulloutput
BCPrice
BCPrice$bpi
debugSource('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
source('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
BCPrice
BCPrice$bpi
colnames(BCPrice$bpi)
BCPrice$bpi[1,]
BCPrice$bpi[1]
BCPrice$bpi[,1]
names(BCPrice$bpi)
names(BCPrice$bpi)<-seq(as.Date("2017-10-10")-2, as.Date("2017-10-10"), by="days")
names(BCPrice$bpi)
names(BCPrice$bpi)<-seq(as.Date("2017-10-17")-2, as.Date("2017-10-17"), by="days")
names(BCPrice$bpi)
source('C:/Localwork/Prototyping/SentimentMining/Sentimine/MainScrape.R')
